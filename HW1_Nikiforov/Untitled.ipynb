{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    \n",
    "def get_clear_text(url):\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    \n",
    "    # Удаление элементов \"скрипт\" и \"стиль\". У всех сайтов, которые проверял, не было элементов \"стиль\", но пускай будет.\n",
    "    # UPD: а нет, всё же есть.\n",
    "    for script_style in soup([\"script\", \"style\"]):\n",
    "        script_style.extract()\n",
    "        \n",
    "    # Проверка показала, что get_text() не считывает содержимое эелементов \"script\" и \"style\",\n",
    "    # так что большая ли необходимость в удалении этих эелементов???     \n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Разбить на строки без пустых пространств.\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    \n",
    "    # Разделение линий с 2-мя и более пробелами.\n",
    "    clear_lines = (phrase.strip() for line in lines for phrase in line.split(r'\\s{2,}'))\n",
    "    \n",
    "    # Объединяем все линии так, что пустые строки исчезают.\n",
    "    text = '\\n'.join(clear_line for clear_line in clear_lines if clear_line)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "urls=set()\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    urls = set()\n",
    "    \n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    \n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            continue\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            if href not in external_urls:\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "    \n",
    "total_urls_visited = 0\n",
    "url_counter=0\n",
    "\n",
    "\n",
    "def crawler(url, max_urls=100):\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    links = get_all_website_links(url)\n",
    "    if len(links) < max_urls:\n",
    "        for link in links:\n",
    "            total_urls_visited += len(links) \n",
    "            if total_urls_visited > max_urls:\n",
    "                break\n",
    "            crawler(link, max_urls=max_urls)\n",
    "    for link in links:\n",
    "        global url_counter\n",
    "        if url_counter==150:\n",
    "            break\n",
    "        url_counter+=1\n",
    "        urls.add(link)\n",
    "        \n",
    "page_counter=0\n",
    "     \n",
    "    \n",
    "def write_txt(text, url, index_txt):\n",
    "    global page_counter\n",
    "    f = open(f'index{page_counter}.txt','w', encoding=\"utf-8\") \n",
    "    \n",
    "    # Выкачиваем текст в отдельный файл.\n",
    "    f.write(text) \n",
    "    f.close()\n",
    "    # Заполняем index.txt новой строкой.\n",
    "    index_txt.write(f'{page_counter}:%s\\n' % url)      \n",
    "    page_counter+=1\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    n=get_clear_text(url)\n",
    "    index_txt = open(f'index.txt','w', encoding=\"utf-8\")\n",
    "    write_txt(n,url, index_txt)\n",
    "    crawler(url)\n",
    "\n",
    "    for u in urls:\n",
    "        text=get_clear_text(u)\n",
    "        \n",
    "        # На случай, если на странице нет текста.\n",
    "        if len(text)==0:\n",
    "            continue\n",
    "        write_txt(text,u, index_txt)\n",
    "    index_txt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
